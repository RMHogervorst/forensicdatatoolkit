---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# forensicdatatoolkit

<!-- badges: start -->
<!-- badges: end -->

The goal of forensicdatatoolkit is to combine several tools created for investigating 
summary data and detect anomalies. This is not a tool for finding fraud. Fraud is a legal term.
These tools merely tell you if the underlying data is probable or not. It can never tell us
why the statistics are wrong. Someone could lie or someone could mistype a number. Only open data
can tell us which it is.

## Installation

You can install the released version of forensicdatatoolkit from [CRAN](https://CRAN.R-project.org) with:

``` r
install.packages("forensicdatatoolkit")
```

## Loading package


```{r example}
library(forensicdatatoolkit)
```


## Techniques available

What is in the package (citations in separate file) [LINK to full citations vignette]

- [x] Granularity-Related Inconsistency of Means (GRIM)
- [] Granularity-Related Inconsistency of Means Mapped to Error Repeats (GRIMMER)
- [X] Sample Parameter Reconstruction via Iterative Techniques (SPRITE)


We can use these tools to investigate summary statistics. 
For example if we collect statistics from a set of papers we can investigate them.

Let's imagin we have 2 published papers about food. The author let several people taste chocolate bars and judge those bars. We use: 

* a 1-5 scale for food acceptance
* a 1-9 scale for how dark the chocolate was and,
* a 1-7 scale for if you like the chocolate

The study only reports the summary statistics (means and standard deviations) for every scale.

```{r}
example_data <- data.frame(
  study = c(1, 1, 1, 2, 2, 2),
  description = rep(c("food acceptance", "darkness", "likeability"), 2),
  n = c(20, 19, 19, 40, 41, 40),
  means = c(3.50, 5.60, 2.13, 2.54, 8.00, 3.05),
  sds = c(1.34, 1.34, 2.56, 0.45, 2, 1),
  scale_min = rep(c(1, 1, 1), 2),
  scale_max = rep(c(5, 9, 7))
)
example_data
```

It is possible that the researcher dropped some chocolate on his work when the values were
being calculated. Fortunately we can do a granularity check on the means to see if those means are 
possible given the scale used, and the sample size. This check is known as the 
GRIM test (Granularity-Related Inconsistency of Means). [LINK TO PREPRINT HERE].

### GRIM (Granularity-Related Inconsistency of Means)
Let's use an example: 20 people each picked an integer between 1 and 5 (because you cannot choose someting else). What happens to the mean score if one person went from 1: disgusting to 2:awful? 
the mean score would go up by 1/20th. The minimal stepsize is 1/20th or 0.05. That means that every mean must be divisable by that stepsize. 3.5 is therefore fine but 3.53 is not.


```{r base r style}
example_data$mean_possible <- grim(example_data$means, example_data$n)
example_data[, c("study", "description", "mean_possible")]
```

<details>
<summary> This also works with tidyverse verbs **click here to unhide code** </summary>
```{r tidyverse style, eval=FALSE}
library(dplyr)
example_data %>%
  mutate(
    mean_possible = grim(mean = means, n = n)
  ) %>%
  select(study, description, mean_possible)
```

</details>

## SPRITE (Sample Parameter Reconstruction via Iterative TEchniques)
[Link to preprint here]

It is also possible to work out reasonable data based on means, standard deviations and minimum and maximum of scale. 

> In other words, for any given mean, we shuffle the available values (very quickly) until we generate a sample with the parameters we’re interested in. Then we do it again, and again, and again. We find hundreds or thousands of plausible solutions. We can model them further if we want.

if want to check what the data could look like.

Example from [James Heathers](https://medium.com/hackernoon/introducing-sprite-and-the-case-of-the-carthorse-child-58683c2bfeb) 

The data comes from “Attractive Names Sustain Increased Vegetable Intake in Schools” by Wansink et.al. (2012).
Mean=19.4, SD=19.9, n=45
Let’s add a fact: you can’t have less than zero carrots (there are no negative carrots, this isn’t Star Trek).

```{r}
library(ggplot2) # because we like pretty graphs
samples <- get_sprite_samples(max_cases = 10, n=45,mean = 19.4,sd=19.9,scale_min = 1, scale_max = 1000)
result <- forensicdatatoolkit:::sprite_into_df(samples) # turn matrix into sample_nr
ggplot(result, aes(answer, group=result_id))+
  geom_histogram()+
  facet_wrap(~result_id)+
  labs(
    title = "Baby carrots eaten",
    subtitle = "by a child, according to this study",
    x = "number of carrots eaten",
    y = "frequency"
  )
```
```{r}
library(dplyr)
result %>% group_by(result_id) %>% 
  summarize(max_carrots=max(answer)) %>% 
  ggplot(aes(max_carrots))+
  geom_histogram()
```


CORVIDS ?


<details>
<summary> At the moment of creation (when I knitted this document ) this was the state of my machine: **click here to expand** </summary>

```{r}
sessioninfo::session_info()
```

</details>
